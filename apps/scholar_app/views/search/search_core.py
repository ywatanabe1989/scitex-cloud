#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# File: /home/ywatanabe/proj/scitex-cloud/apps/scholar_app/views/search/search_core.py
# Auto-generated by refactoring script
# ----------------------------------------
from __future__ import annotations
import os

__FILE__ = "./apps/scholar_app/views/search/search_core.py"
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------
from django.shortcuts import render
from django.http import JsonResponse
from django.views.decorators.http import require_http_methods
from django.contrib.auth.decorators import login_required
from django.views.decorators.csrf import csrf_exempt
from django.core.files.storage import default_storage
from django.core.cache import cache
from django.db.models import Q, Count, Avg, Max, Min
from django.utils import timezone
import json
import requests
import hashlib
from scitex import logging
import asyncio
from datetime import datetime, timedelta
from ...models import (
    SearchIndex, UserLibrary, Author, Journal,
    Collection, Topic, Annotation, AnnotationVote,
    CollaborationGroup, GroupMembership,
    AnnotationTag, UserPreference,
)
from apps.project_app.services import get_current_project

# Import from other search modules
from .search_helpers import extract_search_filters, search_database_papers, apply_advanced_filters, get_paper_authors
from .engines import search_papers_online
from .storage import store_search_result

logger = logging.getLogger(__name__)

# Import scitex.scholar if available
try:
    from scitex.scholar.pipelines.ScholarPipelineSearchParallel import ScholarPipelineSearchParallel
    SCITEX_SCHOLAR_AVAILABLE = True
except ImportError:
    SCITEX_SCHOLAR_AVAILABLE = False




def simple_search_with_tab(
    request, active_tab="search", template_name="scholar_app/index.html"
):
    """Advanced search interface with tab specification."""
    query = request.GET.get("q", "").strip()
    project = request.GET.get("project", "")
    # Handle multiple checkbox source selection
    selected_sources = []
    if request.GET.get("source_pubmed"):
        selected_sources.append("pubmed")
    if request.GET.get("source_google_scholar"):
        selected_sources.append("google_scholar")
    if request.GET.get("source_arxiv"):
        selected_sources.append("arxiv")
    if request.GET.get("source_semantic"):
        selected_sources.append("semantic")
    sources = ",".join(selected_sources) if selected_sources else "all"
    sort_by = request.GET.get("sort_by", "relevance")

    # Check for API key alerts if user is authenticated
    missing_api_keys = []
    if request.user.is_authenticated:
        user_prefs = UserPreference.get_or_create_for_user(request.user)
        missing_api_keys = user_prefs.get_missing_api_keys()

    # Extract advanced filters from request
    filters = extract_search_filters(request)
    results = []

    # If there's a query, search for papers
    if query:
        # First check existing papers in our database with filters applied
        existing_papers = search_database_papers(query, filters)

        # Perform web search for additional results with filters
        user_prefs = None
        if request.user.is_authenticated:
            user_prefs = UserPreference.get_or_create_for_user(request.user)

        web_results = search_papers_online(
            query, sources=sources, filters=filters, user_preferences=user_prefs
        )

        # Combine and store results
        all_results = []

        # Add existing papers
        for paper in existing_papers:
            all_results.append(
                {
                    "id": str(paper.id),
                    "title": paper.title,
                    "authors": get_paper_authors(paper),
                    "year": paper.publication_date.year
                    if paper.publication_date
                    else "Unknown",
                    "journal": paper.journal.name
                    if paper.journal
                    else "Unknown Journal",
                    "citation_count": paper.citation_count,
                    "citation_source": paper.citation_source,
                    "is_open_access": paper.is_open_access,
                    "snippet": paper.abstract[:200] + "..."
                    if paper.abstract
                    else "No abstract available.",
                    "full_abstract": paper.abstract or "",
                    "pdf_url": paper.pdf_url,
                    "external_url": paper.external_url or "",
                    "doi": paper.doi or "",
                    "pmid": paper.pmid or "",
                    "arxiv_id": paper.arxiv_id or "",
                    "impact_factor": paper.journal.impact_factor
                    if paper.journal
                    else None,
                    "source": paper.source,
                    "source_engines": paper.source_engines
                    if paper.source_engines
                    else [],
                }
            )

        # Add web search results
        for result in web_results:
            # Store in database for future searches
            stored_paper = store_search_result(result)
            all_results.append(
                {
                    "id": str(stored_paper.id),
                    "title": result["title"],
                    "authors": result["authors"]
                    if isinstance(result["authors"], str)
                    else ", ".join(result["authors"]),
                    "year": result["year"],
                    "journal": result["journal"],
                    "citation_count": result.get("citation_count", 0),
                    "citation_source": result.get("citation_source", ""),
                    "is_open_access": result.get("is_open_access", False),
                    "snippet": result.get("abstract", "No abstract available.")[:200]
                    + "...",
                    "full_abstract": result.get("abstract", ""),
                    "pdf_url": result.get("pdf_url", ""),
                    "external_url": result.get("external_url", ""),
                    "doi": result.get("doi", ""),
                    "pmid": result.get("pmid", ""),
                    "arxiv_id": result.get("arxiv_id", ""),
                    "impact_factor": result.get("impact_factor"),
                    "source": result.get("source", "web"),
                    "source_engines": result.get("source_engines", []),
                }
            )

        # Apply advanced filters to results
        all_results = apply_advanced_filters(all_results, filters)

        # Apply sorting
        if sort_by == "date":
            all_results.sort(key=lambda x: int(x.get("year", 0)), reverse=True)
        elif sort_by == "citations":
            all_results.sort(key=lambda x: int(x.get("citations", 0)), reverse=True)
        elif sort_by == "relevance":
            # Keep original order (already sorted by relevance from APIs)
            pass

        results = all_results[:10000]  # Return up to 10k results

    # Calculate dynamic filter ranges from results
    filter_ranges = {
        "year_min": 1900,
        "year_max": 2025,
        "citations_min": 0,
        "citations_max": 12000,  # Default fallback
        "impact_factor_min": 0,
        "impact_factor_max": 50.0,  # Default fallback
    }

    if results:
        # Extract publication years (filter out None values)
        years = [r.get("year") for r in results if r.get("year") is not None]
        if years:
            filter_ranges["year_min"] = min(years)
            filter_ranges["year_max"] = max(years)

        # Extract citation counts (filter out None values)
        citation_counts = [
            r.get("citations", 0) for r in results if r.get("citations") is not None
        ]
        if citation_counts:
            filter_ranges["citations_max"] = max(citation_counts)

        # Extract impact factors (filter out None values)
        impact_factors = [
            r.get("impact_factor", 0)
            for r in results
            if r.get("impact_factor") is not None
        ]
        if impact_factors:
            filter_ranges["impact_factor_max"] = max(impact_factors)

    # Get user projects for BibTeX enrichment form and determine current project
    user_projects = []
    current_project = None
    if request.user.is_authenticated:
        from apps.project_app.models import Project

        user_projects = Project.objects.filter(owner=request.user).order_by(
            "-created_at"
        )

        # Determine current project (use same logic as writer_app and scholar_app elsewhere)
        # Try session-based project selection first
        current_project_slug = request.session.get("current_project_slug")
        if current_project_slug:
            try:
                current_project = Project.objects.get(
                    slug=current_project_slug, owner=request.user
                )
            except Project.DoesNotExist:
                pass

        # Fallback: try profile's last active repository
        if (
            not current_project
            and hasattr(request.user, "profile")
            and request.user.profile.last_active_repository
        ):
            current_project = request.user.profile.last_active_repository

        # Fallback: use first project if available
        if not current_project and user_projects.exists():
            current_project = user_projects.first()

    context = {
        "query": query,
        "project": project,
        "current_project": current_project,  # Add current project for BibTeX save functionality
        "sources": sources,
        "sort_by": sort_by,
        "results": results,
        "has_results": bool(results),
        "missing_api_keys": missing_api_keys,
        "user_projects": user_projects,
        "active_tab": active_tab,  # Indicate which tab is active
        "filter_ranges": filter_ranges,  # Add dynamic filter ranges
    }

    return render(request, template_name, context)



