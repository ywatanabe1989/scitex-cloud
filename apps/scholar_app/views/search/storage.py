#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# File: /home/ywatanabe/proj/scitex-cloud/apps/scholar_app/views/search/storage.py
# Auto-generated by refactoring script
# ----------------------------------------
from __future__ import annotations
import os

__FILE__ = "./apps/scholar_app/views/search/storage.py"
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------
from django.shortcuts import render
from django.http import JsonResponse
from django.views.decorators.http import require_http_methods
from django.contrib.auth.decorators import login_required
from django.views.decorators.csrf import csrf_exempt
from django.core.files.storage import default_storage
from django.core.cache import cache
from django.db.models import Q, Count, Avg, Max, Min
from django.utils import timezone
import json
import requests
import hashlib
from scitex import logging
import asyncio
from datetime import datetime, timedelta
from ...models import (
    SearchIndex, UserLibrary, Author, Journal,
    Collection, Topic, Annotation, AnnotationVote,
    CollaborationGroup, GroupMembership,
    AnnotationTag, UserPreference,
)
from apps.project_app.services import get_current_project
from ...models import AuthorPaper

logger = logging.getLogger(__name__)

# Import scitex.scholar if available
try:
    from scitex.scholar.pipelines.ScholarPipelineSearchParallel import ScholarPipelineSearchParallel
    SCITEX_SCHOLAR_AVAILABLE = True
except ImportError:
    SCITEX_SCHOLAR_AVAILABLE = False


def store_search_result(result):
    """Store search result in database with deduplication."""
    try:
        # Check for existing paper using multiple identifiers
        existing_paper = None

        # Check by DOI first (most reliable)
        if result.get("doi"):
            existing_paper = SearchIndex.objects.filter(doi=result["doi"]).first()

        # Check by PMID
        if not existing_paper and result.get("pmid"):
            existing_paper = SearchIndex.objects.filter(pmid=result["pmid"]).first()

        # Check by arXiv ID
        if not existing_paper and result.get("arxiv_id"):
            existing_paper = SearchIndex.objects.filter(
                arxiv_id=result["arxiv_id"]
            ).first()

        # Check by title similarity (exact match or very close)
        if not existing_paper and result.get("title"):
            title_lower = result["title"].lower().strip()
            # Check for exact title match
            existing_paper = SearchIndex.objects.filter(
                title__iexact=title_lower
            ).first()

            # Check for very similar titles (remove common words and check)
            if not existing_paper:
                title_clean = " ".join(
                    [word for word in title_lower.split() if len(word) > 3]
                )
                if (
                    len(title_clean) > 20
                ):  # Only check similarity for substantial titles
                    similar_papers = SearchIndex.objects.filter(
                        title__icontains=title_clean[:30]
                    )
                    for similar_paper in similar_papers:
                        if abs(len(similar_paper.title) - len(result["title"])) < 10:
                            existing_paper = similar_paper
                            break

        # If paper exists, update it with new information
        if existing_paper:
            logger.info(f"Found existing paper, updating: {existing_paper.title}")

            # Update source_engines list (merge sources)
            result_sources = result.get("source_engines", [])
            if isinstance(result_sources, list) and result_sources:
                existing_sources = (
                    existing_paper.source_engines
                    if existing_paper.source_engines
                    else []
                )
                # Merge and deduplicate sources
                merged_sources = list(set(existing_sources + result_sources))
                existing_paper.source_engines = merged_sources
            elif result.get("source"):
                # Fall back to single source if source_engines not provided
                existing_sources = (
                    existing_paper.source_engines
                    if existing_paper.source_engines
                    else []
                )
                source = result.get("source", "unknown")
                if source not in existing_sources:
                    existing_paper.source_engines = existing_sources + [source]

            # Update citation count if new source provides better data
            new_citation_count, is_reliable = validate_citation_count(
                result.get("citations", 0), result.get("source", "unknown")
            )
            if is_reliable and new_citation_count > existing_paper.citation_count:
                existing_paper.citation_count = new_citation_count
                existing_paper.citation_source = result.get("source", "unknown")
                existing_paper.citation_last_updated = timezone.now()

            # Update missing fields
            if not existing_paper.abstract and result.get("abstract"):
                existing_paper.abstract = result["abstract"]
            if not existing_paper.pdf_url and result.get("pdf_url"):
                existing_paper.pdf_url = result["pdf_url"]
            if not existing_paper.doi and result.get("doi"):
                existing_paper.doi = (
                    result["doi"] or None
                )  # Convert empty strings to None
            if not existing_paper.pmid and result.get("pmid"):
                existing_paper.pmid = (
                    result["pmid"] or None
                )  # Convert empty strings to None
            if not existing_paper.arxiv_id and result.get("arxiv_id"):
                existing_paper.arxiv_id = (
                    result["arxiv_id"] or None
                )  # Convert empty strings to None

            # Update authors if existing paper has no authors
            if not existing_paper.authors.exists() and result.get("authors"):
                _create_paper_authors(
                    paper=existing_paper, authors_str=result["authors"]
                )

            existing_paper.save()
            return existing_paper

        # Create or get journal
        journal = None
        if result.get("journal"):
            journal, created = Journal.objects.get_or_create(
                name=result["journal"],
                defaults={"abbreviation": result["journal"][:10]},
            )

        # Prepare source_engines list
        source_engines = result.get("source_engines", [])
        if not source_engines and result.get("source"):
            source_engines = [result.get("source", "web")]

        # Create new paper
        paper = SearchIndex.objects.create(
            title=result["title"],
            abstract=result.get("abstract", ""),
            publication_date=datetime(int(result.get("year", 2024)), 1, 1),
            journal=journal,
            pdf_url=result.get("pdf_url", ""),
            doi=result.get("doi") or None,  # Convert empty strings to None
            pmid=result.get("pmid") or None,  # Convert empty strings to None
            arxiv_id=result.get("arxiv_id") or None,  # Convert empty strings to None
            citation_count=validate_citation_count(
                result.get("citations", 0), result.get("source", "unknown")
            )[0],
            citation_source=result.get("source", "unknown"),
            citation_last_updated=timezone.now()
            if result.get("citations", 0) > 0
            else None,
            is_open_access=result.get("is_open_access", False),
            source=result.get("source", "web"),
            source_engines=source_engines,
            relevance_score=1.0,
        )

        # Create authors
        if result.get("authors"):
            _create_paper_authors(paper=paper, authors_str=result["authors"])

        return paper

    except Exception as e:
        print(f"Error storing search result: {e}")
        # Return a minimal paper object if storage fails
        return SearchIndex.objects.create(
            title=result.get("title", "Unknown Title"),
            abstract=result.get("abstract", ""),
            doi=None,  # Ensure unique constraint compliance
            pmid=None,  # Ensure unique constraint compliance
            arxiv_id=None,  # Ensure unique constraint compliance
            relevance_score=1.0,
        )




def _create_paper_authors(paper, authors_str):
    """Helper function to create author associations for a paper.

    Args:
        paper: SearchIndex paper object
        authors_str: String of comma-separated authors or list of author names
    """
    if not authors_str:
        return

    # Handle both string and list inputs
    if isinstance(authors_str, str):
        author_names = authors_str.split(", ")
    elif isinstance(authors_str, list):
        author_names = authors_str
    else:
        return

    for i, author_name in enumerate(author_names):
        if author_name and author_name.strip():
            # Simple name parsing
            name_parts = author_name.strip().split()
            first_name = name_parts[0] if name_parts else ""
            last_name = " ".join(name_parts[1:]) if len(name_parts) > 1 else ""

            author, created = Author.objects.get_or_create(
                first_name=first_name, last_name=last_name, defaults={"email": ""}
            )

            # Link author to paper
            from ...models import AuthorPaper

            AuthorPaper.objects.get_or_create(
                author=author, paper=paper, defaults={"author_order": i + 1}
            )



