#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# File: /home/ywatanabe/proj/scitex-cloud/apps/scholar_app/views/search/engines/openaccess.py
# Auto-generated by refactoring script - search engines split
# ----------------------------------------
from __future__ import annotations
import os

__FILE__ = "./apps/scholar_app/views/search/engines/openaccess.py"
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------
from django.shortcuts import render
from django.http import JsonResponse
from django.views.decorators.http import require_http_methods
from django.core.cache import cache
import json
import requests
from scitex import logging
import asyncio
from datetime import datetime, timedelta
from ....models import SearchIndex, Journal, Author
from ..citations import get_journal_impact_factor, get_pubmed_citations, validate_citation_count
from ..search_helpers import search_database_papers, get_paper_authors
from ..storage import store_search_result

logger = logging.getLogger(__name__)

try:
    from scitex.scholar.pipelines.ScholarPipelineSearchParallel import ScholarPipelineSearchParallel
    SCITEX_SCHOLAR_AVAILABLE = True
except ImportError:
    SCITEX_SCHOLAR_AVAILABLE = False

def search_doaj(query, max_results=50, filters=None):
    """Search Directory of Open Access Journals (DOAJ)."""
    try:
        # DOAJ API v2 endpoint
        base_url = "https://doaj.org/api/v2/search/articles"
        params = {
            "q": query,
            "pageSize": min(max_results, 50),  # Reduced limit
            "sort": "score:desc",
        }

        response = requests.get(base_url, params=params, timeout=15)
        response.raise_for_status()
        data = response.json()

        results = []
        if "results" in data:
            for i, article in enumerate(data["results"][:max_results]):
                bibjson = article.get("bibjson", {})

                # Extract authors
                authors = []
                if "author" in bibjson:
                    for author in bibjson["author"][:3]:
                        name = author.get("name", "Unknown Author")
                        authors.append(name)

                # Extract journal info
                journal_info = bibjson.get("journal", {})
                journal_name = journal_info.get("title", "DOAJ Journal")

                results.append(
                    {
                        "title": bibjson.get("title", f"DOAJ Article {i + 1}"),
                        "authors": ", ".join(authors) if authors else "DOAJ Authors",
                        "year": str(bibjson.get("year", 2024)),
                        "journal": journal_name,
                        "abstract": bibjson.get(
                            "abstract", f"Open access article about {query} from DOAJ"
                        ),
                        "pdf_url": "",  # DOAJ doesn't always provide direct PDF links
                        "is_open_access": True,
                        "citations": 0,  # DOAJ doesn't provide citation counts
                        "source": "doaj",
                    }
                )

        return results
    except Exception as e:
        print(f"Error searching DOAJ: {e}")
        return []





def search_biorxiv(query, max_results=50, filters=None):
    """Search bioRxiv preprint server."""
    try:
        # bioRxiv API
        base_url = "https://api.biorxiv.org/details/biorxiv"

        # Search recent papers (bioRxiv API is date-based)
        from datetime import datetime, timedelta

        end_date = datetime.now()
        start_date = end_date - timedelta(days=365)  # Last year

        date_str = f"{start_date.strftime('%Y-%m-%d')}/{end_date.strftime('%Y-%m-%d')}"
        url = f"{base_url}/{date_str}"

        response = requests.get(url, timeout=15)
        response.raise_for_status()
        data = response.json()

        results = []
        if "collection" in data:
            # Filter by query and take max_results
            filtered_papers = []
            for paper in data["collection"]:
                title = paper.get("title", "").lower()
                abstract = paper.get("abstract", "").lower()
                if query.lower() in title or query.lower() in abstract:
                    filtered_papers.append(paper)
                    if len(filtered_papers) >= max_results:
                        break

            for i, paper in enumerate(filtered_papers):
                results.append(
                    {
                        "title": paper.get("title", f"bioRxiv Preprint {i + 1}"),
                        "authors": paper.get("authors", "bioRxiv Authors"),
                        "year": paper.get("date", "2024")[:4],
                        "journal": "bioRxiv",
                        "abstract": paper.get(
                            "abstract", f"bioRxiv preprint about {query}"
                        ),
                        "pdf_url": f"https://www.biorxiv.org/content/{paper.get('doi', '')}.full.pdf",
                        "is_open_access": True,
                        "citations": 0,
                        "source": "biorxiv",
                    }
                )

        return results
    except Exception as e:
        print(f"Error searching bioRxiv: {e}")
        return []





def search_plos(query, max_results=50, filters=None):
    """Search PLOS journals (PLOS ONE, PLOS Biology, etc.)."""
    try:
        # PLOS Search API
        base_url = "https://api.plos.org/search"
        params = {
            "q": f'title:"{query}" OR abstract:"{query}"',
            "wt": "json",
            "rows": max_results,
            "sort": "score desc",
            "fl": "id,title,author,journal,publication_date,abstract",
        }

        response = requests.get(base_url, params=params, timeout=15)
        response.raise_for_status()
        data = response.json()

        results = []
        if "response" in data and "docs" in data["response"]:
            for i, doc in enumerate(data["response"]["docs"]):
                # Extract authors
                authors = []
                if "author" in doc:
                    if isinstance(doc["author"], list):
                        authors = doc["author"][:3]
                    else:
                        authors = [doc["author"]]

                # Extract year from publication_date
                pub_date = doc.get("publication_date", "2024-01-01T00:00:00Z")
                year = pub_date[:4] if pub_date else "2024"

                results.append(
                    {
                        "title": doc.get("title", [f"PLOS Article {i + 1}"])[0]
                        if isinstance(doc.get("title"), list)
                        else doc.get("title", f"PLOS Article {i + 1}"),
                        "authors": ", ".join(authors) if authors else "PLOS Authors",
                        "year": year,
                        "journal": doc.get("journal", "PLOS Journal"),
                        "abstract": doc.get(
                            "abstract", [f"PLOS article about {query}"]
                        )[0]
                        if isinstance(doc.get("abstract"), list)
                        else doc.get("abstract", f"PLOS article about {query}"),
                        "pdf_url": f"https://journals.plos.org/plosone/article/file?id={doc.get('id', '')}&type=printable",
                        "is_open_access": True,
                        "citations": 0,  # PLOS API doesn't provide citation counts directly
                        "source": "plos",
                    }
                )

        return results
    except Exception as e:
        print(f"Error searching PLOS: {e}")
        return []





