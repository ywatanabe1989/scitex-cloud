#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# File: /home/ywatanabe/proj/scitex-cloud/apps/scholar_app/views/search/engines/arxiv.py
# Auto-generated by refactoring script - search engines split
# ----------------------------------------
from __future__ import annotations
import os

__FILE__ = "./apps/scholar_app/views/search/engines/arxiv.py"
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------
from django.shortcuts import render
from django.http import JsonResponse
from django.views.decorators.http import require_http_methods
from django.core.cache import cache
import json
import requests
from scitex import logging
import asyncio
from datetime import datetime, timedelta
from ....models import SearchIndex, Journal, Author
from ..citations import get_journal_impact_factor, get_pubmed_citations, validate_citation_count
from ..search_helpers import search_database_papers, get_paper_authors
from ..storage import store_search_result

logger = logging.getLogger(__name__)

try:
    from scitex.scholar.pipelines.ScholarPipelineSearchParallel import ScholarPipelineSearchParallel
    SCITEX_SCHOLAR_AVAILABLE = True
except ImportError:
    SCITEX_SCHOLAR_AVAILABLE = False

def search_arxiv_real(query, max_results=15, filters=None):
    """Real arXiv search that parses actual paper metadata from arXiv API."""
    try:
        logger.info(f"ðŸ” REAL arXiv API search for: '{query}'")

        # Build search query
        search_query = f"all:{query}"
        if filters and filters.get("authors"):
            for author in filters["authors"][:2]:
                search_query += f' AND au:"{author}"'

        base_url = "http://export.arxiv.org/api/query"
        params = {
            "search_query": search_query,
            "start": 0,
            "max_results": min(max_results, 15),
            "sortBy": "relevance",
            "sortOrder": "descending",
        }

        logger.info(f"   Requesting: {base_url} with query: {search_query}")
        response = requests.get(base_url, params=params, timeout=10)
        response.raise_for_status()

        # Parse XML response
        import xml.etree.ElementTree as ET

        root = ET.fromstring(response.content)

        results = []
        namespace = {"atom": "http://www.w3.org/2005/Atom"}
        entries = root.findall("atom:entry", namespace)

        logger.info(f"   Found {len(entries)} arXiv entries")

        for entry in entries:
            try:
                # Extract paper metadata
                title_elem = entry.find("atom:title", namespace)
                authors = entry.findall("atom:author", namespace)
                published_elem = entry.find("atom:published", namespace)
                summary_elem = entry.find("atom:summary", namespace)
                id_elem = entry.find("atom:id", namespace)

                if not title_elem or not id_elem:
                    continue

                # Clean up title (remove extra whitespace/newlines)
                title = " ".join(title_elem.text.strip().split())

                # Extract author names
                author_names = []
                for author in authors:
                    name_elem = author.find("atom:name", namespace)
                    if name_elem and name_elem.text:
                        author_names.append(name_elem.text.strip())

                # Extract publication year
                year = "2024"
                if published_elem and published_elem.text:
                    try:
                        year = published_elem.text[:4]
                    except (IndexError, AttributeError, TypeError):
                        # Failed to extract year from published element, use default
                        pass

                # Extract arXiv ID
                arxiv_url = id_elem.text
                arxiv_id = (
                    arxiv_url.split("/")[-1]
                    .replace("v1", "")
                    .replace("v2", "")
                    .replace("v3", "")
                )

                # Clean up abstract
                abstract = ""
                if summary_elem and summary_elem.text:
                    abstract = " ".join(summary_elem.text.strip().split())[:300] + "..."

                result = {
                    "title": title,
                    "authors": ", ".join(author_names[:3])
                    + (" et al." if len(author_names) > 3 else ""),
                    "year": year,
                    "journal": "arXiv preprint",
                    "abstract": abstract,
                    "pdf_url": f"https://arxiv.org/pdf/{arxiv_id}.pdf",
                    "external_url": f"https://arxiv.org/abs/{arxiv_id}",
                    "arxiv_id": arxiv_id,
                    "doi": "",
                    "pmid": "",
                    "is_open_access": True,
                    "citations": 0,  # arXiv doesn't provide citation counts
                    "source": "arxiv",
                }

                results.append(result)
                logger.info(f"   âœ“ Parsed: {title[:60]}...")

            except Exception as e:
                logger.warning(f"   Failed to parse arXiv entry: {e}")
                continue

        logger.info(f"   Returning {len(results)} real arXiv results")
        return results

    except Exception as e:
        logger.error(f"Real arXiv search failed: {e}")
        return []





def search_arxiv(query, max_results=50, filters=None):
    """Search arXiv for papers with advanced filtering."""
    try:
        # Build search query with filters
        search_query = f"all:{query}"

        # Add author filter to arXiv query if specified
        if filters and filters.get("authors"):
            for author in filters["authors"]:
                search_query += f' AND au:"{author}"'

        # Add year filter to arXiv query if specified
        if filters and (filters.get("year_from") or filters.get("year_to")):
            if filters.get("year_from"):
                search_query += f" AND submittedDate:[{filters['year_from']}0101* TO *]"
            if filters.get("year_to"):
                search_query += f" AND submittedDate:[* TO {filters['year_to']}1231*]"

        base_url = "http://export.arxiv.org/api/query"
        params = {
            "search_query": search_query,
            "start": 0,
            "max_results": max_results,
            "sortBy": "relevance",
            "sortOrder": "descending",
        }

        response = requests.get(base_url, params=params, timeout=10)
        response.raise_for_status()

        # Parse XML response (simplified)
        import xml.etree.ElementTree as ET

        root = ET.fromstring(response.content)

        results = []
        namespace = {"atom": "http://www.w3.org/2005/Atom"}

        for entry in root.findall("atom:entry", namespace):
            title = entry.find("atom:title", namespace)
            authors = entry.findall("atom:author", namespace)
            published = entry.find("atom:published", namespace)
            summary = entry.find("atom:summary", namespace)
            pdf_link = None

            # Find PDF link
            for link in entry.findall("atom:link", namespace):
                if link.get("title") == "pdf":
                    pdf_link = link.get("href")
                    break

            if title is not None:
                author_names = []
                for author in authors:
                    name = author.find("atom:name", namespace)
                    if name is not None:
                        author_names.append(name.text)

                year = "2024"
                if published is not None:
                    try:
                        year = published.text[:4]
                    except (IndexError, AttributeError, TypeError):
                        # Failed to extract year from published element, use default
                        pass

                results.append(
                    {
                        "title": title.text.strip(),
                        "authors": ", ".join(author_names[:3]),  # Limit to 3 authors
                        "year": year,
                        "journal": "arXiv preprint",
                        "abstract": summary.text.strip() if summary is not None else "",
                        "pdf_url": pdf_link,
                        "is_open_access": True,
                        "citations": 0,
                        "source": "arxiv",
                    }
                )

        return results

    except Exception as e:
        print(f"Error searching arXiv: {e}")
        return []





