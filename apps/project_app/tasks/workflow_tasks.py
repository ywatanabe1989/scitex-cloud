#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Celery Tasks for Workflow Execution

GitHub Actions-style workflow execution engine.
"""

from celery import shared_task
from django.utils import timezone
import yaml
import subprocess
import logging
import os

logger = logging.getLogger(__name__)


@shared_task(bind=True, max_retries=3)
def execute_workflow_run(self, run_id):
    """
    Execute a complete workflow run

    Args:
        run_id: WorkflowRun ID

    Returns:
        Dict with execution results
    """
    from apps.project_app.models import WorkflowRun, WorkflowJob

    try:
        run = WorkflowRun.objects.select_related("workflow", "workflow__project").get(
            id=run_id
        )
    except WorkflowRun.DoesNotExist:
        logger.error(f"WorkflowRun {run_id} not found")
        return {"error": "Run not found"}

    logger.info(f"Starting workflow run {run.workflow.name} #{run.run_number}")

    # Update run status
    run.status = "in_progress"
    run.started_at = timezone.now()
    run.save()

    try:
        # Parse workflow YAML
        workflow_config = yaml.safe_load(run.workflow.yaml_content)

        if "jobs" not in workflow_config:
            raise ValueError("Workflow must contain 'jobs' section")

        # Create jobs from YAML
        jobs_config = workflow_config["jobs"]
        jobs_order = list(jobs_config.keys())

        for job_id in jobs_order:
            job_config = jobs_config[job_id]

            # Create WorkflowJob
            job = WorkflowJob.objects.create(
                run=run,
                name=job_config.get("name", job_id),
                job_id=job_id,
                runs_on=job_config.get("runs-on", "ubuntu-latest"),
                depends_on=job_config.get("needs", []),
                status="queued",
            )

            logger.info(f"Created job {job.name} for run {run.id}")

        # Execute jobs in order (respecting dependencies)
        executed_jobs = set()
        failed_jobs = set()

        while len(executed_jobs) + len(failed_jobs) < len(jobs_order):
            # Find jobs ready to execute
            ready_jobs = []

            for job_id in jobs_order:
                if job_id in executed_jobs or job_id in failed_jobs:
                    continue

                job = WorkflowJob.objects.get(run=run, job_id=job_id)
                dependencies = job.depends_on

                # Check if all dependencies are satisfied
                if all(dep in executed_jobs for dep in dependencies):
                    ready_jobs.append(job)

            if not ready_jobs:
                # No jobs ready - check if we're deadlocked
                if len(executed_jobs) + len(failed_jobs) < len(jobs_order):
                    logger.error(f"Workflow deadlock detected in run {run.id}")
                    break
                else:
                    break

            # Execute ready jobs (can be parallelized in future)
            for job in ready_jobs:
                result = execute_workflow_job.delay(job.id)

                # Wait for job completion (blocking for now)
                job_result = result.get(timeout=3600)  # 1 hour timeout

                if job_result.get("success"):
                    executed_jobs.add(job.job_id)
                else:
                    failed_jobs.add(job.job_id)

                    # Check if workflow should continue on error
                    if not job_config.get("continue-on-error", False):
                        logger.error(f"Job {job.name} failed, stopping workflow")
                        break

        # Calculate final status
        all_jobs = WorkflowJob.objects.filter(run=run)
        successful_jobs = all_jobs.filter(conclusion="success").count()
        failed_jobs_count = all_jobs.filter(conclusion="failure").count()
        total_jobs = all_jobs.count()

        if failed_jobs_count > 0:
            run.conclusion = "failure"
        elif successful_jobs == total_jobs:
            run.conclusion = "success"
        else:
            run.conclusion = "cancelled"

        run.status = "completed"
        run.completed_at = timezone.now()
        run.calculate_duration()

        # Update workflow statistics
        workflow = run.workflow
        workflow.total_runs += 1
        if run.conclusion == "success":
            workflow.successful_runs += 1
        elif run.conclusion == "failure":
            workflow.failed_runs += 1
        workflow.last_run_at = run.completed_at
        workflow.last_run_status = run.conclusion
        workflow.save()

        logger.info(f"Workflow run {run.id} completed with status {run.conclusion}")

        return {
            "success": True,
            "run_id": run.id,
            "conclusion": run.conclusion,
            "duration": run.duration_seconds,
        }

    except Exception as e:
        logger.error(f"Error executing workflow run {run.id}: {e}", exc_info=True)

        run.status = "failed"
        run.conclusion = "failure"
        run.completed_at = timezone.now()
        run.calculate_duration()
        run.save()

        return {
            "success": False,
            "error": str(e),
        }


@shared_task(bind=True, max_retries=3)
def execute_workflow_job(self, job_id):
    """
    Execute a single workflow job

    Args:
        job_id: WorkflowJob ID

    Returns:
        Dict with execution results
    """
    from apps.project_app.models import WorkflowJob, WorkflowStep

    try:
        job = WorkflowJob.objects.select_related(
            "run", "run__workflow", "run__workflow__project"
        ).get(id=job_id)
    except WorkflowJob.DoesNotExist:
        logger.error(f"WorkflowJob {job_id} not found")
        return {"error": "Job not found"}

    logger.info(f"Starting job {job.name} for run {job.run.id}")

    # Update job status
    job.status = "in_progress"
    job.started_at = timezone.now()
    job.save()

    try:
        # Parse workflow YAML to get job configuration
        workflow_config = yaml.safe_load(job.run.workflow.yaml_content)
        job_config = workflow_config["jobs"][job.job_id]

        # Get steps
        steps_config = job_config.get("steps", [])

        # Create WorkflowStep objects
        for step_number, step_config in enumerate(steps_config, start=1):
            step_name = step_config.get("name", f"Step {step_number}")
            step_command = step_config.get("run", step_config.get("uses", ""))

            step = WorkflowStep.objects.create(
                job=job,
                name=step_name,
                step_number=step_number,
                command=step_command,
                working_directory=step_config.get("working-directory", ""),
                environment_vars=step_config.get("env", {}),
                condition=step_config.get("if", ""),
                continue_on_error=step_config.get("continue-on-error", False),
                status="queued",
            )

            logger.info(f"Created step {step.name} for job {job.id}")

        # Execute steps sequentially
        steps = WorkflowStep.objects.filter(job=job).order_by("step_number")
        failed_step = None

        for step in steps:
            # Check condition (simplified - just check for 'always()')
            if step.condition and step.condition != "always()":
                # Skip conditional steps for now
                step.status = "skipped"
                step.save()
                continue

            # Execute step
            result = execute_workflow_step.delay(step.id)
            step_result = result.get(timeout=3600)  # 1 hour timeout

            if not step_result.get("success"):
                failed_step = step
                if not step.continue_on_error:
                    logger.error(f"Step {step.name} failed, stopping job")
                    break

        # Calculate job conclusion
        all_steps = steps.count()
        successful_steps = steps.filter(conclusion="success").count()
        failed_steps = steps.filter(conclusion="failure").count()

        if failed_steps > 0:
            job.conclusion = "failure"
        elif successful_steps == all_steps:
            job.conclusion = "success"
        else:
            job.conclusion = "skipped"

        job.status = "completed"
        job.completed_at = timezone.now()
        job.calculate_duration()
        job.save()

        logger.info(f"Job {job.id} completed with conclusion {job.conclusion}")

        return {
            "success": job.conclusion == "success",
            "job_id": job.id,
            "conclusion": job.conclusion,
            "duration": job.duration_seconds,
        }

    except Exception as e:
        logger.error(f"Error executing job {job.id}: {e}", exc_info=True)

        job.status = "failed"
        job.conclusion = "failure"
        job.completed_at = timezone.now()
        job.calculate_duration()
        job.save()

        return {
            "success": False,
            "error": str(e),
        }


@shared_task(bind=True, max_retries=3)
def execute_workflow_step(self, step_id):
    """
    Execute a single workflow step

    Args:
        step_id: WorkflowStep ID

    Returns:
        Dict with execution results
    """
    from apps.project_app.models import WorkflowStep
    from apps.project_app.services.project_filesystem import (
        get_project_filesystem_manager,
    )

    try:
        step = WorkflowStep.objects.select_related(
            "job", "job__run", "job__run__workflow", "job__run__workflow__project"
        ).get(id=step_id)
    except WorkflowStep.DoesNotExist:
        logger.error(f"WorkflowStep {step_id} not found")
        return {"error": "Step not found"}

    logger.info(f"Starting step {step.name} for job {step.job.id}")

    # Update step status
    step.status = "in_progress"
    step.started_at = timezone.now()
    step.save()

    try:
        project = step.job.run.workflow.project

        # Get project directory
        manager = get_project_filesystem_manager(project.owner)
        project_path = manager.get_project_root_path(project)

        if not project_path or not project_path.exists():
            raise FileNotFoundError(f"Project directory not found: {project_path}")

        # Determine working directory
        if step.working_directory:
            working_dir = project_path / step.working_directory
        else:
            working_dir = project_path

        # Prepare environment variables
        env = os.environ.copy()
        env.update(step.environment_vars)

        # Add common environment variables
        env.update(
            {
                "GITHUB_WORKSPACE": str(project_path),
                "GITHUB_REPOSITORY": f"{project.owner.username}/{project.slug}",
                "GITHUB_RUN_ID": str(step.job.run.id),
                "GITHUB_RUN_NUMBER": str(step.job.run.run_number),
                "GITHUB_JOB": step.job.job_id,
            }
        )

        # Execute command
        if step.command.startswith("uses:"):
            # Action reference (e.g., actions/checkout@v3)
            # For now, we'll skip these and just log
            step.output = f"Action {step.command} would be executed here"
            step.exit_code = 0
            step.conclusion = "success"
        else:
            # Shell command
            logger.info(f"Executing command: {step.command}")

            process = subprocess.Popen(
                step.command,
                shell=True,
                cwd=str(working_dir),
                env=env,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
            )

            # Stream output
            stdout_lines = []
            stderr_lines = []

            # Read stdout
            for line in process.stdout:
                stdout_lines.append(line)
                logger.debug(f"STDOUT: {line.strip()}")

            # Read stderr
            for line in process.stderr:
                stderr_lines.append(line)
                logger.debug(f"STDERR: {line.strip()}")

            # Wait for process to complete
            exit_code = process.wait()

            # Save output
            step.output = "".join(stdout_lines)
            step.error_output = "".join(stderr_lines)
            step.exit_code = exit_code

            if exit_code == 0:
                step.conclusion = "success"
            else:
                step.conclusion = "failure"

        step.status = "completed"
        step.completed_at = timezone.now()
        step.calculate_duration()
        step.save()

        logger.info(f"Step {step.id} completed with exit code {step.exit_code}")

        return {
            "success": step.conclusion == "success",
            "step_id": step.id,
            "exit_code": step.exit_code,
            "conclusion": step.conclusion,
            "duration": step.duration_seconds,
        }

    except Exception as e:
        logger.error(f"Error executing step {step.id}: {e}", exc_info=True)

        step.status = "failed"
        step.conclusion = "failure"
        step.error_output = str(e)
        step.exit_code = -1
        step.completed_at = timezone.now()
        step.calculate_duration()
        step.save()

        return {
            "success": False,
            "error": str(e),
        }


# EOF
